{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Review Analytics: Integrating Sentiment Analysis with Yelp Ratings\n",
    "\n",
    "This notebook will take you through the steps from downloading the Yelp Dataset all the way to saving the model locally, to be used in the TR-API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the Yelp Dataset\n",
    "The Yelp Dataset has already been formatted from the original JSON to a readable CSV. \n",
    "It can be downloaded here: https://drive.google.com/file/d/1zPROW4EuuVXwLP8qtwbkuKY3PhFq0rTo/view?usp=sharing\n",
    "\n",
    "See steps to download the original dataset here: [ link ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from joblib import dump\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Partitioning (4,724,472 max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random subset of 100000 records saved to reviews100.0k.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'restaurant_reviews.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Select a random subset of x records\n",
    "partition_size = 100000\n",
    "subset_data = data.sample(n=partition_size, random_state=42) \n",
    "\n",
    "# Save to a new CSV file\n",
    "output_file_path = f'reviews{partition_size/1000}k.csv'\n",
    "subset_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Random subset of {partition_size} records saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dataset Preprocessing\n",
    "This includes the following preprocessing steps:\n",
    "\n",
    "#### General Preprocessing\n",
    "1. Lowercasing\n",
    "2. Removing non-word characters\n",
    "3. Removing extra spaces\n",
    "4. Tokenizing\n",
    "5. Lemmatizing\n",
    "\n",
    "#### NLTK Resource Loading\n",
    "6. punkt, wordnet, and stopwords loading + wordnet lemmatizing\n",
    "\n",
    "#### Final Steps\n",
    "7. Dataset memory loading\n",
    "8. Binarizing lables based on star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLTK resources...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wnr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/wnr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/wnr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "Binarizing labels into five categories...\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words if word not in stop_words])\n",
    "\n",
    "# Load NLTK resources\n",
    "print(\"Loading NLTK resources...\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_csv(output_file_path)\n",
    "\n",
    "# Preprocess the text\n",
    "print(\"Preprocessing text data...\")\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Binarize the labels into five categories\n",
    "def binarize_label(star_rating):\n",
    "    if star_rating == 1:\n",
    "        return 'Very Negative'\n",
    "    elif star_rating == 2:\n",
    "        return 'Negative'\n",
    "    elif star_rating == 3:\n",
    "        return 'Neutral'\n",
    "    elif star_rating == 4:\n",
    "        return 'Positive'\n",
    "    else:  # star_rating == 5\n",
    "        return 'Very Positive'\n",
    "\n",
    "print(\"Binarizing labels into five categories...\")\n",
    "data['sentiment'] = data['stars'].apply(binarize_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Vectorizer & Model (LR) Initialization\n",
    "This step initialized the vectorizer as well as the Logistic Regression Model used in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Dataset Split & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training. Accuracy: 0.64265\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Negative       0.43      0.31      0.36      1766\n",
      "      Neutral       0.47      0.35      0.40      2319\n",
      "     Positive       0.51      0.45      0.48      4769\n",
      "Very Negative       0.69      0.77      0.73      2366\n",
      "Very Positive       0.74      0.86      0.80      8780\n",
      "\n",
      "     accuracy                           0.64     20000\n",
      "    macro avg       0.57      0.55      0.55     20000\n",
      " weighted avg       0.62      0.64      0.63     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing...\")\n",
    "\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "y = data['sentiment']  # Using the new five-category sentiment labels\n",
    "\n",
    "# Splitting, training, and evaluating\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(f\"Finished Training. Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Saving the vectorizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer saved to vectorizer.joblib\n",
      "Final model saved to LRM.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model_file_path = 'LRM.joblib'\n",
    "dump(model, model_file_path)\n",
    "\n",
    "# Save the vectorizer to a file\n",
    "vectorizer_file_path = 'vectorizer.joblib'\n",
    "dump(vectorizer, vectorizer_file_path)\n",
    "\n",
    "print(f\"Vectorizer saved to {vectorizer_file_path}\")\n",
    "\n",
    "print(f\"Final model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Testing the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and vectorizer\n",
    "model_file_path = 'LRM.joblib'\n",
    "vectorizer_file_path = 'vectorizer.joblib'  # Ensure this path is correct\n",
    "model = load(model_file_path)\n",
    "vectorizer = load(vectorizer_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(output_file_path)\n",
    "\n",
    "# Ask the user to specify the number of reviews they want to analyze\n",
    "num_reviews = int(input(\"Enter the number of reviews to analyze: \"))\n",
    "\n",
    "# Select a random subset of reviews\n",
    "sample_data = data.sample(n=num_reviews, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This is more of a quiet relaxing place. My friends and I were looking for something more loud fun and lively so we weren't as impressed. We only ordered hookah which was pretty okay. The decor is gorgeous. The waiters are all wearing costume-like Moroccan uniforms and  are professional but seemed a little tired and distant. If you are looking for a quiet place and willing to pay not-so-cheap prices then this is a nice place. closes by 2am fridays/saturdays.\n",
      "Predicted Sentiment: 0\n",
      "Star Rating: 3.0\n",
      "Subjectivity Score: 0.6092592592592593\n",
      "Adjusted Aggregate Score: 0.0\n",
      "------------------------\n",
      "\n",
      "Review: If I could give this place negative stars I would. I ordered carry out, rib bones, Mac and cheese and green beans. The green beans and mac were flavorless as well as the bones. The only way I could get the bones down was to drown them in BBQ sauce. Everything was so bland except for the bread. Good BBQ doesn't require any sauce whatsoever. If you think this is good BBQ I suggest you try Kings Ribs or City BBQ or, hell make them at home just don't waste your money here.\n",
      "Predicted Sentiment: -2\n",
      "Star Rating: 1.0\n",
      "Subjectivity Score: 0.48024691358024696\n",
      "Adjusted Aggregate Score: -1.519753086419753\n",
      "------------------------\n",
      "\n",
      "Review: The service was great, but the food was lacking. Not being drinkers I think my family and I didn't fit in too well... But the place has a great atmosphere even with kids.\n",
      "Predicted Sentiment: 1\n",
      "Star Rating: 2.0\n",
      "Subjectivity Score: 0.6333333333333333\n",
      "Adjusted Aggregate Score: -0.31666666666666665\n",
      "------------------------\n",
      "\n",
      "Review: Chicken is good but do take out.  My most recent dine in visit I had to wait quite a long time for the chicken to be done.  Meanwhile for delivery they tell me it'll be ready in 15-20 min.  The inconsistency in wait time just isn't acceptable.  The service (or lack of thereof unless you ring the buzzer) just makes me miss Bonchon back in the DC area.\n",
      "Predicted Sentiment: 0\n",
      "Star Rating: 3.0\n",
      "Subjectivity Score: 0.5187499999999999\n",
      "Adjusted Aggregate Score: 0.0\n",
      "------------------------\n",
      "\n",
      "Review: I had really hoped for better from the reviews. The shop is located on a corner of Dickerson pike and I was very glad my boyfriend was with me, the area was sketchy and as a young woman I would not have felt safe going alone. The parking lot was swimming in smushed garbage and the restaurants had metal bars on their windows. This is not to fault the restaurant at all, but again, as a young female I would want to know these factors before coming here alone at night. \n",
      "\n",
      "The service was kind and quick which was greatly appreciated. The prices can't be beat, a quart and a pint of fried rice for around $13. As far as the taste... I couldn't even bring myself to save these leftovers. My chicken was slimy and squishy, but not pink at lessr. The rice was just yellow rice with no seasoning whatsoever and a few carrots and peas thrown in (maybe 15 of both total).\n",
      "Predicted Sentiment: 0\n",
      "Star Rating: 2.0\n",
      "Subjectivity Score: 0.4854166666666666\n",
      "Adjusted Aggregate Score: -0.5\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob  # Assuming TextBlob is used for subjectivity analysis\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Calculate subjectivity scores\n",
    "sample_data['subjectivity'] = sample_data['text'].apply(get_subjectivity)\n",
    "\n",
    "def get_sentiment_score(sentiment):\n",
    "    sentiment_scores = {'Very Negative': -2, 'Negative': -1, 'Neutral': 0, 'Positive': 1, 'Very Positive': 2}\n",
    "    return sentiment_scores[sentiment]\n",
    "\n",
    "# Preprocess, predict sentiment, and calculate aggregate score\n",
    "sample_data['processed_text'] = sample_data['text'].apply(preprocess_text)\n",
    "X_sample = vectorizer.transform(sample_data['processed_text'])\n",
    "sample_data['predicted_sentiment_label'] = model.predict(X_sample)\n",
    "sample_data['predicted_sentiment_score'] = sample_data['predicted_sentiment_label'].apply(get_sentiment_score)\n",
    "\n",
    "def calculate_adjusted_aggregate_score(sentiment_score, star_rating, subjectivity_score):\n",
    "    normalized_star = (star_rating - 3)  # Normalize star rating\n",
    "    weight = 1 - subjectivity_score if sentiment_score > 0 else 1  # Adjust weight based on sentiment and subjectivity\n",
    "    return (sentiment_score * weight + normalized_star) / 2\n",
    "\n",
    "\n",
    "sample_data['adjusted_aggregate_score'] = sample_data.apply(lambda row: calculate_adjusted_aggregate_score(row['predicted_sentiment_score'], row['stars'], row['subjectivity']), axis=1)\n",
    "\n",
    "# Display results\n",
    "for index, row in sample_data.iterrows():\n",
    "    print(f\"Review: {row['text']}\")\n",
    "    print(f\"Predicted Sentiment: {row['predicted_sentiment_score']}\")\n",
    "    print(f\"Star Rating: {row['stars']}\")\n",
    "    print(f\"Subjectivity Score: {row['subjectivity']}\")\n",
    "    print(f\"Adjusted Aggregate Score: {row['adjusted_aggregate_score']}\\n------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
